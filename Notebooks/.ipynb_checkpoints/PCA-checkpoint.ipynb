{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Data/half_processed_train.csv')\n",
    "test = pd.read_csv('../Data/processes_X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop('time_spent', axis=1)\n",
    "y_train = train['time_spent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "val_X_train, val_X_test, val_y_train, val_y_test = train_test_split(X_train, y_train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val_X_train.columns\n",
    "['Unnamed: 0', 'session_number', 'purchased', 'added_in_cart',\n",
    "       'checked_out', 'Android Tablet', 'Desktop', 'Other', 'Unknown', 'iPad',\n",
    "       'iPhone', 'app', 'August', 'December', 'February', 'January', 'July',\n",
    "       'June', 'March', 'May', 'November', 'October', 'September', 'Monday',\n",
    "       'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '2020',\n",
    "       'xg_op_pred', 'rf_op_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['session_number', 'purchased', 'added_in_cart','checked_out','app','September','xg_op_pred', 'rf_op_pred']\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler  = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_val_X_train = scaler.fit_transform(val_X_train)\n",
    "scaled_val_X_test = scaler.transform(val_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_train = pca.fit_transform(scaled_val_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_test = pca.transform(scaled_val_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "model.add(Dense(4, activation='relu'))\n",
    "\n",
    "model.add(Dense(4, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3800 samples, validate on 1629 samples\n",
      "Epoch 1/250\n",
      "3800/3800 [==============================] - 0s 50us/step - loss: 3701523.7039 - val_loss: 2616475.4929\n",
      "Epoch 2/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3701498.0625 - val_loss: 2616437.9139\n",
      "Epoch 3/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3701431.4671 - val_loss: 2616343.1500\n",
      "Epoch 4/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3701314.7105 - val_loss: 2616215.0971\n",
      "Epoch 5/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3701181.0197 - val_loss: 2616065.8782\n",
      "Epoch 6/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3701011.2500 - val_loss: 2615862.3610\n",
      "Epoch 7/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3700766.5987 - val_loss: 2615561.3688\n",
      "Epoch 8/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3700386.8684 - val_loss: 2615076.4406\n",
      "Epoch 9/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3699777.7632 - val_loss: 2614309.5595\n",
      "Epoch 10/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3698867.0132 - val_loss: 2613199.2336\n",
      "Epoch 11/250\n",
      "3800/3800 [==============================] - 0s 12us/step - loss: 3697557.5789 - val_loss: 2611715.5375\n",
      "Epoch 12/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3695870.7829 - val_loss: 2609732.6406\n",
      "Epoch 13/250\n",
      "3800/3800 [==============================] - 0s 12us/step - loss: 3693688.1053 - val_loss: 2607211.6251\n",
      "Epoch 14/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3690924.0461 - val_loss: 2604048.0495\n",
      "Epoch 15/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3687436.5724 - val_loss: 2600158.0609\n",
      "Epoch 16/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3683173.9112 - val_loss: 2595384.0431\n",
      "Epoch 17/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3678048.9539 - val_loss: 2589601.6965\n",
      "Epoch 18/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3671808.7303 - val_loss: 2582774.7280\n",
      "Epoch 19/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3664453.4539 - val_loss: 2574793.6290\n",
      "Epoch 20/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3655786.3355 - val_loss: 2565549.0183\n",
      "Epoch 21/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3645992.4342 - val_loss: 2554635.8921\n",
      "Epoch 22/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3634621.1020 - val_loss: 2542216.7752\n",
      "Epoch 23/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3621591.7072 - val_loss: 2528543.1277\n",
      "Epoch 24/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3607255.0132 - val_loss: 2513131.1984\n",
      "Epoch 25/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3591108.8257 - val_loss: 2496404.5798\n",
      "Epoch 26/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3573571.8947 - val_loss: 2477940.7263\n",
      "Epoch 27/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3554717.6579 - val_loss: 2457718.3802\n",
      "Epoch 28/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3534099.0461 - val_loss: 2436490.1796\n",
      "Epoch 29/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3512154.7204 - val_loss: 2414248.6819\n",
      "Epoch 30/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3489126.1151 - val_loss: 2391135.9959\n",
      "Epoch 31/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3465606.9145 - val_loss: 2366761.6524\n",
      "Epoch 32/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3441093.2434 - val_loss: 2341919.7917\n",
      "Epoch 33/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3415901.2171 - val_loss: 2317342.4929\n",
      "Epoch 34/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3391068.4243 - val_loss: 2292629.8219\n",
      "Epoch 35/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3366307.3849 - val_loss: 2268455.4409\n",
      "Epoch 36/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3342664.4704 - val_loss: 2244423.2425\n",
      "Epoch 37/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3318812.0033 - val_loss: 2223011.2963\n",
      "Epoch 38/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3297386.7303 - val_loss: 2202136.2380\n",
      "Epoch 39/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3277054.8586 - val_loss: 2182803.3251\n",
      "Epoch 40/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3257993.3717 - val_loss: 2166031.4995\n",
      "Epoch 41/250\n",
      "3800/3800 [==============================] - 0s 12us/step - loss: 3241694.3684 - val_loss: 2150422.8321\n",
      "Epoch 42/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3226032.7105 - val_loss: 2138024.7513\n",
      "Epoch 43/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3213604.7829 - val_loss: 2126481.2262\n",
      "Epoch 44/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3202168.1513 - val_loss: 2117145.8262\n",
      "Epoch 45/250\n",
      "3800/3800 [==============================] - 0s 12us/step - loss: 3192753.8520 - val_loss: 2109036.5008\n",
      "Epoch 46/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3184431.1513 - val_loss: 2102890.8161\n",
      "Epoch 47/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3178014.5230 - val_loss: 2097446.8860\n",
      "Epoch 48/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3172060.7336 - val_loss: 2093237.8546\n",
      "Epoch 49/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3167179.2072 - val_loss: 2089853.5806\n",
      "Epoch 50/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3163234.0132 - val_loss: 2087051.1097\n",
      "Epoch 51/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3159875.5493 - val_loss: 2084382.9131\n",
      "Epoch 52/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3156471.1546 - val_loss: 2082506.7837\n",
      "Epoch 53/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3153759.1645 - val_loss: 2080866.0460\n",
      "Epoch 54/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3151308.2138 - val_loss: 2079331.1090\n",
      "Epoch 55/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3148784.2270 - val_loss: 2078077.8851\n",
      "Epoch 56/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3146632.1579 - val_loss: 2076805.5087\n",
      "Epoch 57/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3144704.7138 - val_loss: 2075709.6597\n",
      "Epoch 58/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3142581.7632 - val_loss: 2074415.5672\n",
      "Epoch 59/250\n",
      "3800/3800 [==============================] - 0s 12us/step - loss: 3140865.5987 - val_loss: 2073427.1690\n",
      "Epoch 60/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3138894.3257 - val_loss: 2072556.7990\n",
      "Epoch 61/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3137192.9572 - val_loss: 2071585.3396\n",
      "Epoch 62/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3135319.9901 - val_loss: 2070851.1826\n",
      "Epoch 63/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3133789.6118 - val_loss: 2069965.2795\n",
      "Epoch 64/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3132103.3059 - val_loss: 2069060.4351\n",
      "Epoch 65/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3130592.4046 - val_loss: 2068329.7435\n",
      "Epoch 66/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3129210.2763 - val_loss: 2067635.0078\n",
      "Epoch 67/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3127631.7533 - val_loss: 2066837.9687\n",
      "Epoch 68/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3126251.6743 - val_loss: 2066186.6410\n",
      "Epoch 69/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3124877.8191 - val_loss: 2065488.7221\n",
      "Epoch 70/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800/3800 [==============================] - 0s 10us/step - loss: 3123594.5230 - val_loss: 2064719.8288\n",
      "Epoch 71/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3122159.7270 - val_loss: 2064076.7310\n",
      "Epoch 72/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3120904.7138 - val_loss: 2063345.9603\n",
      "Epoch 73/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3119751.0164 - val_loss: 2062812.0752\n",
      "Epoch 74/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3118433.6250 - val_loss: 2062082.0790\n",
      "Epoch 75/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3117286.0789 - val_loss: 2061478.5681\n",
      "Epoch 76/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3116128.6579 - val_loss: 2060732.4448\n",
      "Epoch 77/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3115175.4112 - val_loss: 2060206.7604\n",
      "Epoch 78/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3113798.7878 - val_loss: 2059342.3445\n",
      "Epoch 79/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3112772.3355 - val_loss: 2058795.2790\n",
      "Epoch 80/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3111728.2566 - val_loss: 2058180.4798\n",
      "Epoch 81/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3110769.1020 - val_loss: 2057655.4058\n",
      "Epoch 82/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3109651.8520 - val_loss: 2057030.2816\n",
      "Epoch 83/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3108643.5164 - val_loss: 2056470.5850\n",
      "Epoch 84/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3107670.6217 - val_loss: 2055721.6885\n",
      "Epoch 85/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3106857.8865 - val_loss: 2055084.1207\n",
      "Epoch 86/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3105795.7796 - val_loss: 2054625.7636\n",
      "Epoch 87/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3104881.4572 - val_loss: 2053951.0812\n",
      "Epoch 88/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3103915.9507 - val_loss: 2053182.7156\n",
      "Epoch 89/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3102980.5000 - val_loss: 2052585.8624\n",
      "Epoch 90/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3102030.6612 - val_loss: 2051960.8964\n",
      "Epoch 91/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3101152.7368 - val_loss: 2051420.0094\n",
      "Epoch 92/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3100321.6776 - val_loss: 2050871.6830\n",
      "Epoch 93/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3099338.5428 - val_loss: 2050271.6265\n",
      "Epoch 94/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3098524.7533 - val_loss: 2049469.7678\n",
      "Epoch 95/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3097651.7829 - val_loss: 2048865.3781\n",
      "Epoch 96/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3096730.5099 - val_loss: 2048228.1595\n",
      "Epoch 97/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3095988.7961 - val_loss: 2047743.0888\n",
      "Epoch 98/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3095071.6447 - val_loss: 2047015.5568\n",
      "Epoch 99/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3094194.2697 - val_loss: 2046363.0530\n",
      "Epoch 100/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3093403.2368 - val_loss: 2045821.8273\n",
      "Epoch 101/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3092767.2237 - val_loss: 2045273.2327\n",
      "Epoch 102/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3091817.3816 - val_loss: 2044585.2545\n",
      "Epoch 103/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3091116.5197 - val_loss: 2043889.5570\n",
      "Epoch 104/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3090188.2566 - val_loss: 2043333.4455\n",
      "Epoch 105/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3089411.9408 - val_loss: 2042582.1677\n",
      "Epoch 106/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3088622.1974 - val_loss: 2042007.2666\n",
      "Epoch 107/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3087835.4539 - val_loss: 2041483.6918\n",
      "Epoch 108/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3087103.6678 - val_loss: 2040731.6250\n",
      "Epoch 109/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3086419.3158 - val_loss: 2040194.0646\n",
      "Epoch 110/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3085525.5789 - val_loss: 2039426.8022\n",
      "Epoch 111/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3084775.3421 - val_loss: 2038964.7018\n",
      "Epoch 112/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3083998.8289 - val_loss: 2038366.5976\n",
      "Epoch 113/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3083273.8618 - val_loss: 2037770.2260\n",
      "Epoch 114/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3082618.7105 - val_loss: 2037258.3969\n",
      "Epoch 115/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3081784.3783 - val_loss: 2036442.4520\n",
      "Epoch 116/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3081024.6382 - val_loss: 2035958.1147\n",
      "Epoch 117/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3080315.7303 - val_loss: 2035614.5526\n",
      "Epoch 118/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3079629.7467 - val_loss: 2034920.4381\n",
      "Epoch 119/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3078952.7697 - val_loss: 2034306.1857\n",
      "Epoch 120/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3078325.9507 - val_loss: 2033736.2041\n",
      "Epoch 121/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3077597.4803 - val_loss: 2033242.4378\n",
      "Epoch 122/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3077091.6316 - val_loss: 2032605.1401\n",
      "Epoch 123/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3076313.8421 - val_loss: 2032263.6255\n",
      "Epoch 124/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3075698.0789 - val_loss: 2031499.2798\n",
      "Epoch 125/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3075071.0789 - val_loss: 2031067.9221\n",
      "Epoch 126/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3074407.1316 - val_loss: 2030497.0942\n",
      "Epoch 127/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3073872.3684 - val_loss: 2029825.3214\n",
      "Epoch 128/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3073155.9605 - val_loss: 2029451.1690\n",
      "Epoch 129/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3072539.9901 - val_loss: 2028943.3963\n",
      "Epoch 130/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3071935.9243 - val_loss: 2028496.3516\n",
      "Epoch 131/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3071494.0263 - val_loss: 2028171.9693\n",
      "Epoch 132/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3070859.2599 - val_loss: 2027377.6742\n",
      "Epoch 133/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3070213.9112 - val_loss: 2027027.6357\n",
      "Epoch 134/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3069808.7730 - val_loss: 2026548.9407\n",
      "Epoch 135/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3069140.6612 - val_loss: 2026153.1542\n",
      "Epoch 136/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3068591.5033 - val_loss: 2025696.7465\n",
      "Epoch 137/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3067989.4507 - val_loss: 2025243.8985\n",
      "Epoch 138/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3067554.9901 - val_loss: 2024819.0104\n",
      "Epoch 139/250\n",
      "3800/3800 [==============================] - ETA: 0s - loss: 8980660.000 - 0s 10us/step - loss: 3066936.1645 - val_loss: 2024345.9897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3066473.6645 - val_loss: 2023774.1287\n",
      "Epoch 141/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3065902.5954 - val_loss: 2023547.7989\n",
      "Epoch 142/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3065358.1513 - val_loss: 2022997.8588\n",
      "Epoch 143/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3064805.8618 - val_loss: 2022564.8910\n",
      "Epoch 144/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3064453.0329 - val_loss: 2022446.0371\n",
      "Epoch 145/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3063904.9342 - val_loss: 2021645.4139\n",
      "Epoch 146/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3063395.1447 - val_loss: 2021433.9584\n",
      "Epoch 147/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3062804.2039 - val_loss: 2021093.5658\n",
      "Epoch 148/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3062389.6250 - val_loss: 2020702.2618\n",
      "Epoch 149/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3061949.2796 - val_loss: 2020332.2870\n",
      "Epoch 150/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3061561.3651 - val_loss: 2019882.4784\n",
      "Epoch 151/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3060993.4342 - val_loss: 2019538.0396\n",
      "Epoch 152/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3060762.5921 - val_loss: 2019377.9986\n",
      "Epoch 153/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3060231.2105 - val_loss: 2018656.7426\n",
      "Epoch 154/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3059642.5000 - val_loss: 2018382.2822\n",
      "Epoch 155/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3059254.9441 - val_loss: 2018223.2094\n",
      "Epoch 156/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3058874.1349 - val_loss: 2017885.1677\n",
      "Epoch 157/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3058385.7467 - val_loss: 2017646.7011\n",
      "Epoch 158/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3058079.1875 - val_loss: 2017205.6466\n",
      "Epoch 159/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3057529.5789 - val_loss: 2016792.6030\n",
      "Epoch 160/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3057214.5164 - val_loss: 2016442.0558\n",
      "Epoch 161/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3056778.0559 - val_loss: 2016223.4309\n",
      "Epoch 162/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3056431.3947 - val_loss: 2015804.6992\n",
      "Epoch 163/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3055974.2599 - val_loss: 2015570.8764\n",
      "Epoch 164/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3055606.3059 - val_loss: 2015396.8393\n",
      "Epoch 165/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3055166.8092 - val_loss: 2014897.9039\n",
      "Epoch 166/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3054802.4803 - val_loss: 2014610.8566\n",
      "Epoch 167/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3054503.1283 - val_loss: 2014390.5602\n",
      "Epoch 168/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3054094.3914 - val_loss: 2013959.9814\n",
      "Epoch 169/250\n",
      "3800/3800 [==============================] - 0s 9us/step - loss: 3053736.0362 - val_loss: 2013528.3190\n",
      "Epoch 170/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3053395.8947 - val_loss: 2013364.4607\n",
      "Epoch 171/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3053094.1184 - val_loss: 2013040.4514\n",
      "Epoch 172/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3052688.9770 - val_loss: 2012853.5353\n",
      "Epoch 173/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3052352.6316 - val_loss: 2012769.5182\n",
      "Epoch 174/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3052019.2072 - val_loss: 2012332.5818\n",
      "Epoch 175/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3051688.5757 - val_loss: 2011915.7805\n",
      "Epoch 176/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3051397.8421 - val_loss: 2011681.7049\n",
      "Epoch 177/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3051242.0428 - val_loss: 2011441.4145\n",
      "Epoch 178/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3050837.5625 - val_loss: 2011481.0576\n",
      "Epoch 179/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3050487.4539 - val_loss: 2011206.6371\n",
      "Epoch 180/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3050168.6118 - val_loss: 2010947.6064\n",
      "Epoch 181/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3049973.6414 - val_loss: 2010612.3066\n",
      "Epoch 182/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3049646.9408 - val_loss: 2010361.7163\n",
      "Epoch 183/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3049327.7730 - val_loss: 2010281.1661\n",
      "Epoch 184/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3049141.7467 - val_loss: 2010027.3944\n",
      "Epoch 185/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3048880.8355 - val_loss: 2009796.7967\n",
      "Epoch 186/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3048633.9539 - val_loss: 2009438.0351\n",
      "Epoch 187/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3048386.1711 - val_loss: 2009142.4562\n",
      "Epoch 188/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3048124.0757 - val_loss: 2009228.6594\n",
      "Epoch 189/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3047784.7632 - val_loss: 2008987.6131\n",
      "Epoch 190/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3047586.4803 - val_loss: 2008836.9659\n",
      "Epoch 191/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3047364.3388 - val_loss: 2008669.4451\n",
      "Epoch 192/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3047174.0921 - val_loss: 2008709.4947\n",
      "Epoch 193/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3046941.3257 - val_loss: 2008595.3379\n",
      "Epoch 194/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3046659.4211 - val_loss: 2008166.3790\n",
      "Epoch 195/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3046485.6579 - val_loss: 2008042.8322\n",
      "Epoch 196/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3046246.1941 - val_loss: 2007795.7843\n",
      "Epoch 197/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3046172.2007 - val_loss: 2007780.5361\n",
      "Epoch 198/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3045814.7533 - val_loss: 2007378.2307\n",
      "Epoch 199/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3045889.2007 - val_loss: 2007471.0319\n",
      "Epoch 200/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3045439.8224 - val_loss: 2007060.1605\n",
      "Epoch 201/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3045332.8092 - val_loss: 2006803.9608\n",
      "Epoch 202/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3045108.2039 - val_loss: 2006858.6246\n",
      "Epoch 203/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3045119.4507 - val_loss: 2006409.0181\n",
      "Epoch 204/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3044717.3750 - val_loss: 2006473.9883\n",
      "Epoch 205/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3044595.8289 - val_loss: 2006269.6936\n",
      "Epoch 206/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3044336.3586 - val_loss: 2006205.2943\n",
      "Epoch 207/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3044216.3816 - val_loss: 2006179.8046\n",
      "Epoch 208/250\n",
      "3800/3800 [==============================] - 0s 13us/step - loss: 3044028.5592 - val_loss: 2005819.2970\n",
      "Epoch 209/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800/3800 [==============================] - 0s 12us/step - loss: 3043840.5724 - val_loss: 2005707.7842\n",
      "Epoch 210/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3043637.1776 - val_loss: 2005681.9240\n",
      "Epoch 211/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3043498.7533 - val_loss: 2005650.3272\n",
      "Epoch 212/250\n",
      "3800/3800 [==============================] - 0s 12us/step - loss: 3043317.7664 - val_loss: 2005422.8742\n",
      "Epoch 213/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3043200.1086 - val_loss: 2005325.2350\n",
      "Epoch 214/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3042969.0000 - val_loss: 2005186.5540\n",
      "Epoch 215/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3042872.5164 - val_loss: 2005090.7993\n",
      "Epoch 216/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3042544.7072 - val_loss: 2004732.9154\n",
      "Epoch 217/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3042507.9441 - val_loss: 2004537.8174\n",
      "Epoch 218/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3042276.5789 - val_loss: 2004595.6604\n",
      "Epoch 219/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3042094.0230 - val_loss: 2004253.8758\n",
      "Epoch 220/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3041948.5789 - val_loss: 2004332.0904\n",
      "Epoch 221/250\n",
      "3800/3800 [==============================] - 0s 12us/step - loss: 3041767.8684 - val_loss: 2004153.1312\n",
      "Epoch 222/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3041673.7796 - val_loss: 2003955.9698\n",
      "Epoch 223/250\n",
      "3800/3800 [==============================] - 0s 12us/step - loss: 3041430.8388 - val_loss: 2003711.5792\n",
      "Epoch 224/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3041327.3191 - val_loss: 2003677.3691\n",
      "Epoch 225/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3041386.7632 - val_loss: 2003434.9016\n",
      "Epoch 226/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3041030.5329 - val_loss: 2003361.5490\n",
      "Epoch 227/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3040819.5658 - val_loss: 2003182.5515\n",
      "Epoch 228/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3040707.4539 - val_loss: 2002901.5641\n",
      "Epoch 229/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3040653.4605 - val_loss: 2003336.8640\n",
      "Epoch 230/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3040440.5921 - val_loss: 2003066.9982\n",
      "Epoch 231/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3040279.6645 - val_loss: 2002774.1433\n",
      "Epoch 232/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3040341.8980 - val_loss: 2002772.2851\n",
      "Epoch 233/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3040001.6086 - val_loss: 2002530.2858\n",
      "Epoch 234/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3039775.9507 - val_loss: 2002368.8483\n",
      "Epoch 235/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3039731.1414 - val_loss: 2002241.3343\n",
      "Epoch 236/250\n",
      "3800/3800 [==============================] - 0s 9us/step - loss: 3039607.0559 - val_loss: 2002383.8412\n",
      "Epoch 237/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3039406.3553 - val_loss: 2001977.0677\n",
      "Epoch 238/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3039235.4243 - val_loss: 2001988.9424\n",
      "Epoch 239/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3039056.7697 - val_loss: 2001914.2943\n",
      "Epoch 240/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3038910.7105 - val_loss: 2001687.1743\n",
      "Epoch 241/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3038882.9605 - val_loss: 2001489.5717\n",
      "Epoch 242/250\n",
      "3800/3800 [==============================] - 0s 11us/step - loss: 3038758.0362 - val_loss: 2001338.9263\n",
      "Epoch 243/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3038567.6513 - val_loss: 2001115.1330\n",
      "Epoch 244/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3038485.7467 - val_loss: 2001132.0866\n",
      "Epoch 245/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3038246.7270 - val_loss: 2001057.6163\n",
      "Epoch 246/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3038202.1020 - val_loss: 2001028.9434\n",
      "Epoch 247/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3038118.1217 - val_loss: 2000990.9226\n",
      "Epoch 248/250\n",
      "3800/3800 [==============================] - 0s 10us/step - loss: 3037889.8421 - val_loss: 2000719.8277\n",
      "Epoch 249/250\n",
      "3800/3800 [==============================] - 0s 12us/step - loss: 3037763.7599 - val_loss: 2000735.3950\n",
      "Epoch 250/250\n",
      "3800/3800 [==============================] - 0s 13us/step - loss: 3037763.6020 - val_loss: 2000889.3163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2483ea6be80>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(pca_X_train, \n",
    "          val_y_train.values, \n",
    "          epochs=250, \n",
    "          validation_data=(pca_X_test, val_y_test.values), \n",
    "          batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(model.history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_pred = model.predict(pca_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1663149899466627\n",
      "2.198909452422597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_log_error, mean_absolute_error\n",
    "print(np.sqrt(mean_squared_log_error(dnn_pred, val_y_test)))\n",
    "print(np.sqrt(mean_squared_log_error(model.predict(pca_X_train), val_y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
